{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization Seminar\n",
    "\n",
    "Сравнение различных методов оптимизации моделей: TorchScript, ONNX, Pruning, Graph Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.classification import Accuracy, F1Score\n",
    "import sys\n",
    "sys.path.append('../../lesson5/seminar')\n",
    "from text_features_data import TextFeaturesDataModule\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Загрузка датасета\n",
    "\n",
    "Загружаем датасет AG News из модуля text_features_data.py. Датасет содержит TF-IDF векторы и статистические фичи для классификации новостных текстов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing AG News dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train: 100%|██████████| 120000/120000 [00:01<00:00, 86933.57it/s]\n",
      "Loading test: 100%|██████████| 7600/7600 [00:00<00:00, 83117.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total texts: 127600\n",
      "Split: train=108460, val=19140\n",
      "Extracting TF-IDF features (max_features=20000, ngrams=(1, 2))...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF train: 100%|██████████| 108460/108460 [00:03<00:00, 27188.23it/s]\n",
      "TF-IDF val: 100%|██████████| 19140/19140 [00:00<00:00, 29234.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF dimension: 20000\n",
      "Extracting statistical features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Statistical features: 100%|██████████| 108460/108460 [00:03<00:00, 29154.09it/s]\n",
      "Statistical features: 100%|██████████| 19140/19140 [00:00<00:00, 31554.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating train features (108460x20000 + 5)...\n",
      "Concatenating val features (19140x20000 + 5)...\n",
      "Total feature dimension: 20005\n",
      "Normalizing statistical features...\n",
      "Normalization done\n",
      "Saving to cache...\n",
      "Saving train X (108460x20005, 16553.8 MB)...\n",
      "Saving train y...\n",
      "Saving val X (19140x20005, 2921.3 MB)...\n",
      "Saving val y...\n",
      "Saved to cache\n",
      "Dataset ready: classes=4, features=20005, train=108460, val=19140\n",
      "Input dim: 20005\n",
      "Num classes: 4\n",
      "Train samples: 108460\n",
      "Val samples: 19140\n"
     ]
    }
   ],
   "source": [
    "dm = TextFeaturesDataModule(\n",
    "    max_features=20000,\n",
    "    batch_size=128,\n",
    "    use_bigrams=True,\n",
    "    cache_dir=\"./cache\",\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "\n",
    "print(f\"Input dim: {dm.input_dim}\")\n",
    "print(f\"Num classes: {dm.n_classes}\")\n",
    "print(f\"Train samples: {len(dm.train_dataset)}\")\n",
    "print(f\"Val samples: {len(dm.val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Построение Multi-Branch модели\n",
    "\n",
    "Создаем модель с multi-branch архитектурой. Модель содержит три параллельные ветки: Bottleneck, Inverted Bottleneck и Regular блоки. Результаты веток объединяются через конкатенацию.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 8,410,884\n"
     ]
    }
   ],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, dim, activation='gelu', dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.bottleneck_dim = max(dim // 4, 1)\n",
    "        self.activation = nn.GELU() if activation == 'gelu' else nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else None\n",
    "        self.fc1 = nn.Linear(self.dim, self.bottleneck_dim)\n",
    "        self.fc2 = nn.Linear(self.bottleneck_dim, self.dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        if self.dropout is not None:\n",
    "            out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out + identity\n",
    "\n",
    "class InvertedBottleneckBlock(nn.Module):\n",
    "    def __init__(self, dim, expansion_factor=4, activation='gelu', dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.expanded_dim = dim * expansion_factor\n",
    "        self.activation = nn.GELU() if activation == 'gelu' else nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else None\n",
    "        self.fc1 = nn.Linear(self.dim, self.expanded_dim)\n",
    "        self.fc2 = nn.Linear(self.expanded_dim, self.dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        if self.dropout is not None:\n",
    "            out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out + identity\n",
    "\n",
    "class RegularBlock(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim=None, activation='gelu', dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim if hidden_dim else dim * 2\n",
    "        self.activation = nn.GELU() if activation == 'gelu' else nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else None\n",
    "        self.fc1 = nn.Linear(self.dim, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation(out)\n",
    "        if self.dropout is not None:\n",
    "            out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out + identity\n",
    "\n",
    "class MultiBranchMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        output_dim,\n",
    "        num_blocks=4,\n",
    "        dropout=0.1,\n",
    "        combine_mode='concat'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.combine_mode = combine_mode\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.bottleneck_branch = nn.ModuleList([\n",
    "            BottleneckBlock(hidden_dim, dropout=dropout) for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.inverted_branch = nn.ModuleList([\n",
    "            InvertedBottleneckBlock(hidden_dim, dropout=dropout) for _ in range(num_blocks)\n",
    "        ])\n",
    "        self.regular_branch = nn.ModuleList([\n",
    "            RegularBlock(hidden_dim, dropout=dropout) for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        if combine_mode == 'concat':\n",
    "            self.output_proj = nn.Linear(hidden_dim * 3, output_dim)\n",
    "        else:\n",
    "            self.output_proj = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        bottleneck_out = x\n",
    "        for block in self.bottleneck_branch:\n",
    "            bottleneck_out = block(bottleneck_out)\n",
    "        \n",
    "        inverted_out = x\n",
    "        for block in self.inverted_branch:\n",
    "            inverted_out = block(inverted_out)\n",
    "        \n",
    "        regular_out = x\n",
    "        for block in self.regular_branch:\n",
    "            regular_out = block(regular_out)\n",
    "        \n",
    "        if self.combine_mode == 'concat':\n",
    "            combined = torch.cat([bottleneck_out, inverted_out, regular_out], dim=1)\n",
    "        else:\n",
    "            combined = bottleneck_out + inverted_out + regular_out\n",
    "        \n",
    "        out = self.output_proj(combined)\n",
    "        return out\n",
    "\n",
    "model = MultiBranchMLP(\n",
    "    input_dim=dm.input_dim,\n",
    "    hidden_dim=256,\n",
    "    output_dim=dm.n_classes,\n",
    "    num_blocks=4,\n",
    "    dropout=0.1,\n",
    "    combine_mode='concat'\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Обучение модели\n",
    "\n",
    "Обучаем модель с использованием PyTorch Lightning. Используем стандартные настройки оптимизатора и функцию потерь для многоклассовой классификации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache...\n",
      "Dataset loaded: classes=4, features=20005, train=108460, val=19140\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4383d5495ac640d0935b7d28e3463654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "class TextClassificationModule(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-3, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.train_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.val_f1 = F1Score(task='multiclass', num_classes=num_classes, average='macro')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.train_acc(logits, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.val_acc(logits, y)\n",
    "        self.val_f1(logits, y)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_f1', self.val_f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "lightning_model = TextClassificationModule(model, lr=1e-3, num_classes=dm.n_classes)\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=5,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=False,\n",
    "    accelerator='gpu',\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "trainer.fit(lightning_model, dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Измерение метрик и производительности базовой модели\n",
    "\n",
    "Измеряем точность, F1-score, скорость инференса и потребление памяти CPU для базовой PyTorch модели. Используем валидационный датасет для оценки метрик и замеряем время выполнения на CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Results:\n",
      "  Accuracy: 0.9031\n",
      "  F1 Score: 0.9030\n",
      "  Inference Time: 2.8081 s\n",
      "  Throughput: 6816.09 samples/s\n",
      "  Memory Usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "def measure_model_performance(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    process = psutil.Process()\n",
    "    memory_before = process.memory_info().rss / 1024 / 1024\n",
    "    \n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            y_true_all.append(y.cpu().numpy())\n",
    "            y_pred_all.append(preds.cpu().numpy())\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    memory_after = process.memory_info().rss / 1024 / 1024\n",
    "    memory_used = memory_after - memory_before\n",
    "    \n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "    \n",
    "    accuracy_metric = Accuracy(task='multiclass', num_classes=dm.n_classes)\n",
    "    f1_metric = F1Score(task='multiclass', num_classes=dm.n_classes, average='macro')\n",
    "    \n",
    "    accuracy = accuracy_metric(torch.LongTensor(y_pred), torch.LongTensor(y_true)).item()\n",
    "    f1 = f1_metric(torch.LongTensor(y_pred), torch.LongTensor(y_true)).item()\n",
    "    \n",
    "    num_samples = len(y_true)\n",
    "    throughput = num_samples / inference_time\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'inference_time': inference_time,\n",
    "        'throughput': throughput,\n",
    "        'memory_mb': memory_used\n",
    "    }\n",
    "\n",
    "val_loader = dm.val_dataloader()\n",
    "baseline_results = measure_model_performance(model, val_loader, device='cpu')\n",
    "\n",
    "print(f\"Baseline Model Results:\")\n",
    "print(f\"  Accuracy: {baseline_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Score: {baseline_results['f1']:.4f}\")\n",
    "print(f\"  Inference Time: {baseline_results['inference_time']:.4f} s\")\n",
    "print(f\"  Throughput: {baseline_results['throughput']:.2f} samples/s\")\n",
    "print(f\"  Memory Usage: {baseline_results['memory_mb']:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Конвертация в TorchScript\n",
    "\n",
    "Конвертируем модель в TorchScript формат используя torch.jit.script. TorchScript позволяет выполнять модели независимо от Python интерпретатора и может применять различные оптимизации графа вычислений.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchScript Model Results:\n",
      "  Accuracy: 0.9031\n",
      "  F1 Score: 0.9030\n",
      "  Inference Time: 2.0786 s\n",
      "  Throughput: 9208.04 samples/s\n",
      "  Memory Usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "example_input = torch.randn(1, dm.input_dim)\n",
    "\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "torchscript_results = measure_model_performance(scripted_model, val_loader, device='cpu')\n",
    "\n",
    "print(f\"TorchScript Model Results:\")\n",
    "print(f\"  Accuracy: {torchscript_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Score: {torchscript_results['f1']:.4f}\")\n",
    "print(f\"  Inference Time: {torchscript_results['inference_time']:.4f} s\")\n",
    "print(f\"  Throughput: {torchscript_results['throughput']:.2f} samples/s\")\n",
    "print(f\"  Memory Usage: {torchscript_results['memory_mb']:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Конвертация в ONNX\n",
    "\n",
    "Экспортируем модель в формат ONNX используя torch.onnx.export. ONNX является открытым стандартом для представления моделей машинного обучения и позволяет запускать модели в различных runtime окружениях, включая ONNX Runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Results:\n",
      "  Accuracy: 0.9031\n",
      "  F1 Score: 0.9030\n",
      "  Inference Time: 2.0176 s\n",
      "  Throughput: 9486.61 samples/s\n",
      "  Memory Usage: 10.05 MB\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "example_input = torch.randn(1, dm.input_dim)\n",
    "\n",
    "onnx_path = \"model.onnx\"\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    example_input,\n",
    "    onnx_path,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
    "    opset_version=11\n",
    ")\n",
    "\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "def measure_onnx_performance(ort_session, dataloader):\n",
    "    process = psutil.Process()\n",
    "    memory_before = process.memory_info().rss / 1024 / 1024\n",
    "    \n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        x, y = batch\n",
    "        x_np = x.numpy()\n",
    "        \n",
    "        outputs = ort_session.run(None, {'input': x_np})\n",
    "        logits = torch.from_numpy(outputs[0])\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        y_true_all.append(y.numpy())\n",
    "        y_pred_all.append(preds.numpy())\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    memory_after = process.memory_info().rss / 1024 / 1024\n",
    "    memory_used = memory_after - memory_before\n",
    "    \n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "    \n",
    "    accuracy_metric = Accuracy(task='multiclass', num_classes=dm.n_classes)\n",
    "    f1_metric = F1Score(task='multiclass', num_classes=dm.n_classes, average='macro')\n",
    "    \n",
    "    accuracy = accuracy_metric(torch.LongTensor(y_pred), torch.LongTensor(y_true)).item()\n",
    "    f1 = f1_metric(torch.LongTensor(y_pred), torch.LongTensor(y_true)).item()\n",
    "    \n",
    "    num_samples = len(y_true)\n",
    "    throughput = num_samples / inference_time\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'inference_time': inference_time,\n",
    "        'throughput': throughput,\n",
    "        'memory_mb': memory_used\n",
    "    }\n",
    "\n",
    "onnx_results = measure_onnx_performance(ort_session, val_loader)\n",
    "\n",
    "print(f\"ONNX Model Results:\")\n",
    "print(f\"  Accuracy: {onnx_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Score: {onnx_results['f1']:.4f}\")\n",
    "print(f\"  Inference Time: {onnx_results['inference_time']:.4f} s\")\n",
    "print(f\"  Throughput: {onnx_results['throughput']:.2f} samples/s\")\n",
    "print(f\"  Memory Usage: {onnx_results['memory_mb']:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pruning модели\n",
    "\n",
    "Применяем magnitude-based pruning к модели. Pruning удаляет наименее важные веса на основе их абсолютных значений, что позволяет уменьшить размер модели и ускорить инференс при сохранении приемлемой точности.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Model Results:\n",
      "  Accuracy: 0.9033\n",
      "  F1 Score: 0.9032\n",
      "  Inference Time: 2.4471 s\n",
      "  Throughput: 7821.48 samples/s\n",
      "  Memory Usage: 19.85 MB\n",
      "  Parameters: 8,410,884 (original: 8,410,884, reduction: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "pruned_model = MultiBranchMLP(\n",
    "    input_dim=dm.input_dim,\n",
    "    hidden_dim=256,\n",
    "    output_dim=dm.n_classes,\n",
    "    num_blocks=4,\n",
    "    dropout=0.1,\n",
    "    combine_mode='concat'\n",
    ")\n",
    "pruned_model.load_state_dict(model.state_dict())\n",
    "\n",
    "parameters_to_prune = []\n",
    "for name, module in pruned_model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        parameters_to_prune.append((module, 'weight'))\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.3,\n",
    ")\n",
    "\n",
    "for module, name in parameters_to_prune:\n",
    "    prune.remove(module, name)\n",
    "\n",
    "pruned_results = measure_model_performance(pruned_model, val_loader, device='cpu')\n",
    "\n",
    "print(f\"Pruned Model Results:\")\n",
    "print(f\"  Accuracy: {pruned_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Score: {pruned_results['f1']:.4f}\")\n",
    "print(f\"  Inference Time: {pruned_results['inference_time']:.4f} s\")\n",
    "print(f\"  Throughput: {pruned_results['throughput']:.2f} samples/s\")\n",
    "print(f\"  Memory Usage: {pruned_results['memory_mb']:.2f} MB\")\n",
    "\n",
    "pruned_params = sum(p.numel() for p in pruned_model.parameters())\n",
    "original_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  Parameters: {pruned_params:,} (original: {original_params:,}, reduction: {(1 - pruned_params/original_params)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Оптимизация графа вычислений\n",
    "\n",
    "Применяем оптимизации графа вычислений используя torch.jit.optimize_for_inference. Эта функция выполняет различные оптимизации, такие как слияние операций, удаление неиспользуемых веток и другие преобразования для ускорения инференса.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Graph Model Results:\n",
      "  Accuracy: 0.9031\n",
      "  F1 Score: 0.9030\n",
      "  Inference Time: 2.2327 s\n",
      "  Throughput: 8572.60 samples/s\n",
      "  Memory Usage: 6.57 MB\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "example_input = torch.randn(1, dm.input_dim)\n",
    "\n",
    "optimized_model = torch.jit.script(model)\n",
    "optimized_model = torch.jit.optimize_for_inference(optimized_model)\n",
    "\n",
    "optimized_results = measure_model_performance(optimized_model, val_loader, device='cpu')\n",
    "\n",
    "print(f\"Optimized Graph Model Results:\")\n",
    "print(f\"  Accuracy: {optimized_results['accuracy']:.4f}\")\n",
    "print(f\"  F1 Score: {optimized_results['f1']:.4f}\")\n",
    "print(f\"  Inference Time: {optimized_results['inference_time']:.4f} s\")\n",
    "print(f\"  Throughput: {optimized_results['throughput']:.2f} samples/s\")\n",
    "print(f\"  Memory Usage: {optimized_results['memory_mb']:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Сравнение результатов\n",
    "\n",
    "Сравниваем все методы оптимизации по метрикам точности, скорости инференса и потреблению памяти.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of all optimization methods:\n",
      "--------------------------------------------------------------------------------\n",
      "Method               Accuracy     F1 Score     Time (s)     Throughput      Memory (MB)    \n",
      "--------------------------------------------------------------------------------\n",
      "Baseline             0.9031       0.9030       2.8081       6816.09         0.00           \n",
      "TorchScript          0.9031       0.9030       2.0786       9208.04         0.00           \n",
      "ONNX                 0.9031       0.9030       2.0176       9486.61         10.05          \n",
      "Pruned               0.9033       0.9032       2.4471       7821.48         19.85          \n",
      "Optimized Graph      0.9031       0.9030       2.2327       8572.60         6.57           \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'Baseline': baseline_results,\n",
    "    'TorchScript': torchscript_results,\n",
    "    'ONNX': onnx_results,\n",
    "    'Pruned': pruned_results,\n",
    "    'Optimized Graph': optimized_results\n",
    "}\n",
    "\n",
    "print(\"Comparison of all optimization methods:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Method':<20} {'Accuracy':<12} {'F1 Score':<12} {'Time (s)':<12} {'Throughput':<15} {'Memory (MB)':<15}\")\n",
    "print(\"-\" * 80)\n",
    "for method, res in results.items():\n",
    "    print(f\"{method:<20} {res['accuracy']:<12.4f} {res['f1']:<12.4f} {res['inference_time']:<12.4f} {res['throughput']:<15.2f} {res['memory_mb']:<15.2f}\")\n",
    "print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_py10",
   "language": "python",
   "name": "train_py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
