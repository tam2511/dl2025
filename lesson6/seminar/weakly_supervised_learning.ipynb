{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weakly Supervised Learning на датасете Adult\n",
    "\n",
    "В этом ноутбуке исследуются методы обучения со слабой разметкой: шумные метки, частичные метки и агрегированные (bag-level) метки. Сравниваются устойчивые лоссы и специализированные подходы с upper bound на чистых метках.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Подготовка данных\n",
    "\n",
    "Загружаем датасет Adult и применяем ColumnTransformer для обработки числовых и категориальных признаков без утечки информации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (33212, 14), Val: (5861, 14), Test: (9769, 14)\n",
      "Class distribution (train): [25265  7947]\n",
      "Classes: ['<=50K' '>50K']\n"
     ]
    }
   ],
   "source": [
    "adult = fetch_openml('adult', version=2, as_frame=True, parser='pandas')\n",
    "X = adult.data\n",
    "y = adult.target\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.15, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "print(f\"Class distribution (train): {np.bincount(y_train)}\")\n",
    "print(f\"Classes: {le.classes_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension after preprocessing: 108\n"
     ]
    }
   ],
   "source": [
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "input_dim = X_train_processed.shape[1]\n",
    "print(f\"Input dimension after preprocessing: {input_dim}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Базовая модель и метрики\n",
    "\n",
    "Определяем MLP-классификатор и функции для вычисления метрик, включая калибровку (Brier score и ECE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze(-1)\n",
    "\n",
    "def compute_ece(y_true, y_prob, n_bins=10):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    \n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)\n",
    "        prop_in_bin = in_bin.mean()\n",
    "        \n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = y_true[in_bin].mean()\n",
    "            avg_confidence_in_bin = y_prob[in_bin].mean()\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    \n",
    "    return ece\n",
    "\n",
    "def evaluate_model(model, X, y, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X).to(device)\n",
    "        logits = model(X_tensor)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "    \n",
    "    acc = accuracy_score(y, preds)\n",
    "    f1 = f1_score(y, preds, average='macro')\n",
    "    roc_auc = roc_auc_score(y, probs)\n",
    "    pr_auc = average_precision_score(y, probs)\n",
    "    brier = brier_score_loss(y, probs)\n",
    "    ece = compute_ece(y, probs)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'macro_f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'brier': brier,\n",
    "        'ece': ece\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upper Bound: обучение на чистых метках\n",
    "\n",
    "Обучаем модель на чистых метках для получения upper bound — эталона для сравнения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=50, lr=1e-3, \n",
    "                criterion_fn=None, device='cpu', verbose=True):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    if criterion_fn is None:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        criterion = criterion_fn\n",
    "    \n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_train), \n",
    "        torch.FloatTensor(y_train)\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    \n",
    "    best_val_auc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_batch)\n",
    "            \n",
    "            if hasattr(criterion, '__call__'):\n",
    "                loss = criterion(logits, y_batch)\n",
    "            else:\n",
    "                loss = F.binary_cross_entropy_with_logits(logits, y_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        val_metrics = evaluate_model(model, X_val, y_val, device)\n",
    "        \n",
    "        if val_metrics['pr_auc'] > best_val_auc:\n",
    "            best_val_auc = val_metrics['pr_auc']\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, \"\n",
    "                  f\"Val PR-AUC: {val_metrics['pr_auc']:.4f}\")\n",
    "    \n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "============================================================\n",
      "Training Upper Bound (Clean Labels)\n",
      "============================================================\n",
      "Epoch 10/80, Loss: 0.2943, Val PR-AUC: 0.7893\n",
      "Epoch 20/80, Loss: 0.2767, Val PR-AUC: 0.7829\n",
      "Epoch 30/80, Loss: 0.2612, Val PR-AUC: 0.7771\n",
      "Epoch 40/80, Loss: 0.2495, Val PR-AUC: 0.7738\n",
      "Epoch 50/80, Loss: 0.2394, Val PR-AUC: 0.7685\n",
      "Epoch 60/80, Loss: 0.2307, Val PR-AUC: 0.7671\n",
      "Epoch 70/80, Loss: 0.2219, Val PR-AUC: 0.7630\n",
      "Epoch 80/80, Loss: 0.2173, Val PR-AUC: 0.7620\n",
      "\n",
      "Upper Bound Test Metrics:\n",
      "  accuracy: 0.8514\n",
      "  macro_f1: 0.7772\n",
      "  roc_auc: 0.8925\n",
      "  pr_auc: 0.7498\n",
      "  brier: 0.1083\n",
      "  ece: 0.0472\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Upper Bound (Clean Labels)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_clean = MLPClassifier(input_dim, hidden_dim=256)\n",
    "model_clean = train_model(model_clean, X_train_processed, y_train.astype(np.float32), \n",
    "                          X_val_processed, y_val, epochs=80, device=device)\n",
    "\n",
    "clean_metrics = evaluate_model(model_clean, X_test_processed, y_test, device)\n",
    "print(f\"\\nUpper Bound Test Metrics:\")\n",
    "for k, v in clean_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Сценарий A: Шумные метки\n",
    "\n",
    "### 4.1 Генерация шумных меток\n",
    "\n",
    "Создаем симметричный и асимметричный шум в метках:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symmetric noise: 6595 labels flipped (19.9%)\n"
     ]
    }
   ],
   "source": [
    "def add_label_noise(y, noise_type='symmetric', p_flip=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    y_noisy = y.copy()\n",
    "    n = len(y)\n",
    "    \n",
    "    if noise_type == 'symmetric':\n",
    "        flip_mask = np.random.rand(n) < p_flip\n",
    "        y_noisy[flip_mask] = 1 - y_noisy[flip_mask]\n",
    "        \n",
    "        print(f\"Symmetric noise: {flip_mask.sum()} labels flipped ({flip_mask.sum()/n*100:.1f}%)\")\n",
    "        \n",
    "    elif noise_type == 'asymmetric':\n",
    "        p_0_to_1 = p_flip\n",
    "        p_1_to_0 = p_flip / 2\n",
    "        \n",
    "        mask_0 = (y == 0)\n",
    "        mask_1 = (y == 1)\n",
    "        \n",
    "        flip_0 = mask_0 & (np.random.rand(n) < p_0_to_1)\n",
    "        flip_1 = mask_1 & (np.random.rand(n) < p_1_to_0)\n",
    "        \n",
    "        y_noisy[flip_0] = 1\n",
    "        y_noisy[flip_1] = 0\n",
    "        \n",
    "        print(f\"Asymmetric noise: {flip_0.sum()} (0→1), {flip_1.sum()} (1→0)\")\n",
    "    \n",
    "    return y_noisy\n",
    "\n",
    "p_flip = 0.2\n",
    "y_train_noisy = add_label_noise(y_train, noise_type='symmetric', p_flip=p_flip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Устойчивые лоссы\n",
    "\n",
    "Реализуем различные устойчивые к шуму функции потерь:\n",
    "- **Label Smoothing**: $y_{\\text{smooth}} = (1-\\epsilon) \\cdot y + \\epsilon \\cdot 0.5$\n",
    "- **Bootstrapping**: $\\mathcal{L} = \\alpha \\cdot \\text{CE}(y, p) + (1-\\alpha) \\cdot \\text{CE}(p_{\\text{detach}}, p)$\n",
    "- **Confidence Penalty**: $\\mathcal{L} = \\text{CE}(y, p) - \\lambda \\cdot H(p)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        targets_smooth = targets * (1 - self.smoothing) + 0.5 * self.smoothing\n",
    "        return F.binary_cross_entropy_with_logits(logits, targets_smooth)\n",
    "\n",
    "class BootstrappingLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        loss_hard = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "        loss_soft = F.binary_cross_entropy(probs, probs.detach())\n",
    "        return self.alpha * loss_hard + (1 - self.alpha) * loss_soft\n",
    "\n",
    "class ConfidencePenaltyLoss(nn.Module):\n",
    "    def __init__(self, lambda_penalty=0.1):\n",
    "        super().__init__()\n",
    "        self.lambda_penalty = lambda_penalty\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "        entropy = -(probs * torch.log(probs + 1e-8) + \n",
    "                   (1 - probs) * torch.log(1 - probs + 1e-8))\n",
    "        return ce_loss - self.lambda_penalty * entropy.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training with BCE Baseline on Noisy Labels\n",
      "============================================================\n",
      "Test Metrics:\n",
      "  Accuracy: 0.8278, PR-AUC: 0.6345\n",
      "\n",
      "============================================================\n",
      "Training with Label Smoothing on Noisy Labels\n",
      "============================================================\n",
      "Test Metrics:\n",
      "  Accuracy: 0.8221, PR-AUC: 0.6300\n",
      "\n",
      "============================================================\n",
      "Training with Bootstrapping on Noisy Labels\n",
      "============================================================\n",
      "Test Metrics:\n",
      "  Accuracy: 0.8253, PR-AUC: 0.6258\n",
      "\n",
      "============================================================\n",
      "Training with Confidence Penalty on Noisy Labels\n",
      "============================================================\n",
      "Test Metrics:\n",
      "  Accuracy: 0.8248, PR-AUC: 0.6239\n"
     ]
    }
   ],
   "source": [
    "noisy_results = {}\n",
    "\n",
    "loss_configs = [\n",
    "    ('BCE Baseline', None),\n",
    "    ('Label Smoothing', LabelSmoothingLoss(smoothing=0.1)),\n",
    "    ('Bootstrapping', BootstrappingLoss(alpha=0.8)),\n",
    "    ('Confidence Penalty', ConfidencePenaltyLoss(lambda_penalty=0.05))\n",
    "]\n",
    "\n",
    "for name, loss_fn in loss_configs:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with {name} on Noisy Labels\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = MLPClassifier(input_dim, hidden_dim=256)\n",
    "    model = train_model(model, X_train_processed, y_train_noisy.astype(np.float32),\n",
    "                       X_val_processed, y_val, epochs=80, \n",
    "                       criterion_fn=loss_fn, device=device, verbose=False)\n",
    "    \n",
    "    metrics = evaluate_model(model, X_test_processed, y_test, device)\n",
    "    noisy_results[name] = metrics\n",
    "    \n",
    "    print(f\"Test Metrics:\")\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}, PR-AUC: {metrics['pr_auc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Small-Loss Selection\n",
    "\n",
    "Отбираем для обучения только примеры с наименьшим лоссом, фильтруя потенциально зашумленные метки.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training with Small-Loss Selection\n",
      "============================================================\n",
      "Epoch 20, f_ratio: 0.79, selected: 26403/33212, Val PR-AUC: 0.7381\n",
      "Epoch 40, f_ratio: 0.90, selected: 29724/33212, Val PR-AUC: 0.7154\n",
      "Epoch 60, f_ratio: 0.90, selected: 29890/33212, Val PR-AUC: 0.6727\n",
      "Epoch 80, f_ratio: 0.90, selected: 29890/33212, Val PR-AUC: 0.6593\n",
      "\n",
      "Test Metrics:\n",
      "  Accuracy: 0.8327, PR-AUC: 0.6532\n"
     ]
    }
   ],
   "source": [
    "def train_with_small_loss_selection(model, X_train, y_train, X_val, y_val, \n",
    "                                    epochs=80, device='cpu'):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    criterion = LabelSmoothingLoss(smoothing=0.1)\n",
    "    \n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_train), \n",
    "        torch.FloatTensor(y_train)\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=False)\n",
    "    \n",
    "    best_val_auc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        f_ratio = 0.7 + 0.2 * min(1.0, epoch / (epochs // 2))\n",
    "        \n",
    "        all_losses = []\n",
    "        all_indices = []\n",
    "        \n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = model(x_batch)\n",
    "                losses = F.binary_cross_entropy_with_logits(logits, y_batch, reduction='none')\n",
    "                \n",
    "                batch_start = batch_idx * 256\n",
    "                for i, loss_val in enumerate(losses):\n",
    "                    all_losses.append(loss_val.item())\n",
    "                    all_indices.append(batch_start + i)\n",
    "        \n",
    "        sorted_indices = np.argsort(all_losses)\n",
    "        n_select = int(len(sorted_indices) * f_ratio)\n",
    "        selected_indices = sorted_indices[:n_select]\n",
    "        \n",
    "        X_selected = X_train[selected_indices]\n",
    "        y_selected = y_train[selected_indices]\n",
    "        \n",
    "        selected_dataset = TensorDataset(\n",
    "            torch.FloatTensor(X_selected),\n",
    "            torch.FloatTensor(y_selected)\n",
    "        )\n",
    "        selected_loader = DataLoader(selected_dataset, batch_size=256, shuffle=True)\n",
    "        \n",
    "        train_loss = 0\n",
    "        for x_batch, y_batch in selected_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        val_metrics = evaluate_model(model, X_val, y_val, device)\n",
    "        \n",
    "        if val_metrics['pr_auc'] > best_val_auc:\n",
    "            best_val_auc = val_metrics['pr_auc']\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}, f_ratio: {f_ratio:.2f}, selected: {n_select}/{len(X_train)}, \"\n",
    "                  f\"Val PR-AUC: {val_metrics['pr_auc']:.4f}\")\n",
    "    \n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training with Small-Loss Selection\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "model_small_loss = MLPClassifier(input_dim, hidden_dim=256)\n",
    "model_small_loss = train_with_small_loss_selection(\n",
    "    model_small_loss, X_train_processed, y_train_noisy.astype(np.float32),\n",
    "    X_val_processed, y_val, epochs=80, device=device\n",
    ")\n",
    "\n",
    "metrics_small_loss = evaluate_model(model_small_loss, X_test_processed, y_test, device)\n",
    "noisy_results['Small-Loss Selection'] = metrics_small_loss\n",
    "print(f\"\\nTest Metrics:\")\n",
    "print(f\"  Accuracy: {metrics_small_loss['accuracy']:.4f}, PR-AUC: {metrics_small_loss['pr_auc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Consistency Regularization на шумных метках\n",
    "\n",
    "Добавляем consistency regularization (Π-model-lite) для улучшения устойчивости к шуму.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training with Consistency Regularization\n",
      "============================================================\n",
      "Epoch 20, λ_u: 0.95, Val PR-AUC: 0.7613\n",
      "Epoch 40, λ_u: 1.00, Val PR-AUC: 0.7400\n",
      "Epoch 60, λ_u: 1.00, Val PR-AUC: 0.6783\n",
      "Epoch 80, λ_u: 1.00, Val PR-AUC: 0.6515\n",
      "\n",
      "Test Metrics:\n",
      "  Accuracy: 0.8302, PR-AUC: 0.6523\n"
     ]
    }
   ],
   "source": [
    "class WeakAugmentation:\n",
    "    def __init__(self, noise_std=0.05, dropout_prob=0.05):\n",
    "        self.noise_std = noise_std\n",
    "        self.dropout_prob = dropout_prob\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        noise = torch.randn_like(x) * self.noise_std\n",
    "        x_aug = x + noise\n",
    "        mask = torch.rand_like(x) > self.dropout_prob\n",
    "        x_aug = x_aug * mask\n",
    "        return x_aug\n",
    "\n",
    "class StrongAugmentation:\n",
    "    def __init__(self, noise_std=0.15, dropout_prob=0.2, scale_range=0.1):\n",
    "        self.noise_std = noise_std\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.scale_range = scale_range\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        noise = torch.randn_like(x) * self.noise_std\n",
    "        x_aug = x + noise\n",
    "        mask = torch.rand_like(x) > self.dropout_prob\n",
    "        x_aug = x_aug * mask\n",
    "        scale = 1 + (torch.rand_like(x) * 2 - 1) * self.scale_range\n",
    "        x_aug = x_aug * scale\n",
    "        return x_aug\n",
    "\n",
    "def train_with_consistency(model, X_train, y_train, X_val, y_val, \n",
    "                          epochs=80, lambda_u=1.0, rampup_epochs=20, device='cpu'):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    criterion_sup = LabelSmoothingLoss(smoothing=0.1)\n",
    "    \n",
    "    weak_aug = WeakAugmentation()\n",
    "    strong_aug = StrongAugmentation()\n",
    "    \n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_train),\n",
    "        torch.FloatTensor(y_train)\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    \n",
    "    best_val_auc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss_sup = 0\n",
    "        train_loss_unsup = 0\n",
    "        \n",
    "        current_lambda = lambda_u * min(1.0, epoch / rampup_epochs)\n",
    "        \n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            logits = model(x_batch)\n",
    "            loss_sup = criterion_sup(logits, y_batch)\n",
    "            \n",
    "            x_weak = weak_aug(x_batch)\n",
    "            x_strong = strong_aug(x_batch)\n",
    "            \n",
    "            logits_weak = model(x_weak)\n",
    "            logits_strong = model(x_strong)\n",
    "            \n",
    "            probs_weak = torch.sigmoid(logits_weak)\n",
    "            probs_strong = torch.sigmoid(logits_strong)\n",
    "            \n",
    "            loss_unsup = F.mse_loss(probs_weak, probs_strong)\n",
    "            \n",
    "            loss = loss_sup + current_lambda * loss_unsup\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_sup += loss_sup.item()\n",
    "            train_loss_unsup += loss_unsup.item()\n",
    "        \n",
    "        val_metrics = evaluate_model(model, X_val, y_val, device)\n",
    "        \n",
    "        if val_metrics['pr_auc'] > best_val_auc:\n",
    "            best_val_auc = val_metrics['pr_auc']\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}, λ_u: {current_lambda:.2f}, \"\n",
    "                  f\"Val PR-AUC: {val_metrics['pr_auc']:.4f}\")\n",
    "    \n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training with Consistency Regularization\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "model_consistency = MLPClassifier(input_dim, hidden_dim=256)\n",
    "model_consistency = train_with_consistency(\n",
    "    model_consistency, X_train_processed, y_train_noisy.astype(np.float32),\n",
    "    X_val_processed, y_val, epochs=80, device=device\n",
    ")\n",
    "\n",
    "metrics_consistency = evaluate_model(model_consistency, X_test_processed, y_test, device)\n",
    "noisy_results['Consistency Regularization'] = metrics_consistency\n",
    "print(f\"\\nTest Metrics:\")\n",
    "print(f\"  Accuracy: {metrics_consistency['accuracy']:.4f}, PR-AUC: {metrics_consistency['pr_auc']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_py10",
   "language": "python",
   "name": "train_py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
