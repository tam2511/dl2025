{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Создание нейросети с помощью NumPy\n",
        "\n",
        "В этом ноутбуке мы:\n",
        "1. Реализуем простой нейрон используя NumPy\n",
        "2. Вычислим производные для обратного распространения\n",
        "3. Создадим класс слоя, который сохраняет состояния необходимые для прямого и обратного прохода\n",
        "4. Создадим нейросеть из нескольких слоев\n",
        "5. Обучим сеть на реальном датасете (Breast Cancer) для бинарной классификации\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Реализация простого нейрона\n",
        "\n",
        "Нейрон выполняет следующую операцию:\n",
        "$$y = \\sigma(w^T x + b)$$\n",
        "\n",
        "где:\n",
        "- $x$ - входной вектор\n",
        "- $w$ - вектор весов\n",
        "- $b$ - смещение\n",
        "- $\\sigma$ - функция активации (будем использовать сигмоиду)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def neuron_forward(x, w, b):\n",
        "    z = np.dot(w, x) + b\n",
        "    output = sigmoid(z)\n",
        "    return output\n",
        "\n",
        "x = np.array([1.0, 2.0, 3.0])\n",
        "w = np.array([0.5, -0.2, 0.1])\n",
        "b = 0.3\n",
        "\n",
        "output = neuron_forward(x, w, b)\n",
        "print(f\"Input: {x}\")\n",
        "print(f\"Weights: {w}\")\n",
        "print(f\"Bias: {b}\")\n",
        "print(f\"Output: {output:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Вычисление производных для обратного распространения\n",
        "\n",
        "Для обратного распространения нам нужно вычислить градиенты:\n",
        "\n",
        "**Производная сигмоиды:**\n",
        "$$\\frac{\\partial \\sigma}{\\partial z} = \\sigma(z) \\cdot (1 - \\sigma(z))$$\n",
        "\n",
        "**Градиенты по параметрам:**\n",
        "- $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w} = \\frac{\\partial L}{\\partial y} \\cdot \\sigma'(z) \\cdot x$\n",
        "- $\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b} = \\frac{\\partial L}{\\partial y} \\cdot \\sigma'(z)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid_derivative(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def neuron_backward(x, w, b, dy):\n",
        "    z = np.dot(w, x) + b\n",
        "    dz = dy * sigmoid_derivative(z)\n",
        "    dw = dz * x\n",
        "    db = dz\n",
        "    dx = dz * w\n",
        "    return dw, db, dx\n",
        "\n",
        "x = np.array([1.0, 2.0, 3.0])\n",
        "w = np.array([0.5, -0.2, 0.1])\n",
        "b = 0.3\n",
        "dy = 1.0\n",
        "\n",
        "dw, db, dx = neuron_backward(x, w, b, dy)\n",
        "print(f\"Gradient w.r.t. weights: {dw}\")\n",
        "print(f\"Gradient w.r.t. bias: {db:.4f}\")\n",
        "print(f\"Gradient w.r.t. input: {dx}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Класс слоя нейронов с управлением состоянием\n",
        "\n",
        "Теперь создадим класс слоя, который:\n",
        "- Содержит несколько нейронов (принимает n_features входов и возвращает n_outputs выходов)\n",
        "- Хранит параметры (матрицу весов и вектор смещений)\n",
        "- Кэширует промежуточные значения во время прямого прохода\n",
        "- Использует кэшированные значения для эффективного обратного прохода\n",
        "- Реализует обновление параметров\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_outputs = n_outputs\n",
        "        self.W = np.random.randn(n_inputs, n_outputs) * 0.01\n",
        "        self.b = np.zeros(n_outputs)\n",
        "        self.cache = {}\n",
        "        \n",
        "    def forward(self, x):\n",
        "        z = np.dot(x, self.W) + self.b\n",
        "        output = sigmoid(z)\n",
        "        self.cache['x'] = x\n",
        "        self.cache['z'] = z\n",
        "        self.cache['output'] = output\n",
        "        return output\n",
        "    \n",
        "    def backward(self, dy):\n",
        "        x = self.cache['x']\n",
        "        output = self.cache['output']\n",
        "        dz = dy * output * (1 - output)\n",
        "        \n",
        "        if x.ndim == 1:\n",
        "            self.dW = np.outer(x, dz)\n",
        "            self.db = dz\n",
        "            dx = np.dot(dz, self.W.T)\n",
        "        else:\n",
        "            batch_size = x.shape[0]\n",
        "            self.dW = np.dot(x.T, dz) / batch_size\n",
        "            self.db = np.mean(dz, axis=0)\n",
        "            dx = np.dot(dz, self.W.T)\n",
        "        \n",
        "        return dx\n",
        "    \n",
        "    def update_parameters(self, learning_rate):\n",
        "        self.W -= learning_rate * self.dW\n",
        "        self.b -= learning_rate * self.db\n",
        "    \n",
        "    def get_parameters(self):\n",
        "        return {'W': self.W.copy(), 'b': self.b.copy()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Пример использования класса\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "layer = Layer(n_inputs=3, n_outputs=2)\n",
        "\n",
        "x = np.array([1.0, 2.0, 3.0])\n",
        "output = layer.forward(x)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Output: {output}\")\n",
        "\n",
        "dy = np.ones(2)\n",
        "dx = layer.backward(dy)\n",
        "print(f\"Градиент по входу: {dx}\")\n",
        "\n",
        "layer.update_parameters(learning_rate=0.1)\n",
        "params = layer.get_parameters()\n",
        "print(f\"Размер матрицы весов: {params['W'].shape}\")\n",
        "print(f\"Размер вектора смещений: {params['b'].shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Создание нейросети из нескольких слоев\n",
        "\n",
        "Теперь создадим класс нейросети, которая будет:\n",
        "- Состоять из нескольких слоев\n",
        "- Последовательно выполнять прямой проход через все слои\n",
        "- Последовательно выполнять обратный проход через все слои (в обратном порядке)\n",
        "- Обновлять параметры всех слоев\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes):\n",
        "        self.layers = []\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            layer = Layer(layer_sizes[i], layer_sizes[i + 1])\n",
        "            self.layers.append(layer)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        output = x\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(output)\n",
        "        return output\n",
        "    \n",
        "    def backward(self, dy):\n",
        "        dx = dy\n",
        "        for layer in reversed(self.layers):\n",
        "            dx = layer.backward(dx)\n",
        "        return dx\n",
        "    \n",
        "    def update_parameters(self, learning_rate):\n",
        "        for layer in self.layers:\n",
        "            layer.update_parameters(learning_rate)\n",
        "    \n",
        "    def get_all_parameters(self):\n",
        "        params = []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            params.append({\n",
        "                'layer': i,\n",
        "                'parameters': layer.get_parameters()\n",
        "            })\n",
        "        return params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Пример использования нейросети\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "network = NeuralNetwork([4, 8, 3, 1])\n",
        "\n",
        "print(\"Архитектура сети:\")\n",
        "for i, layer in enumerate(network.layers):\n",
        "    print(f\"  Слой {i+1}: {layer.n_inputs} -> {layer.n_outputs}\")\n",
        "\n",
        "x = np.array([1.0, 2.0, 3.0, 4.0])\n",
        "output = network.forward(x)\n",
        "print(f\"\\nВход: {x}\")\n",
        "print(f\"Выход сети: {output}\")\n",
        "\n",
        "dy = np.array([1.0])\n",
        "dx = network.backward(dy)\n",
        "print(f\"\\nГрадиент по входу: {dx}\")\n",
        "\n",
        "network.update_parameters(learning_rate=0.01)\n",
        "print(\"\\nПараметры обновлены!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Обучение на реальном датасете (Breast Cancer)\n",
        "\n",
        "Обучим нашу сеть на датасете Wisconsin Breast Cancer для бинарной классификации опухолей (доброкачественная/злокачественная) по 30 признакам\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target.reshape(-1, 1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Размер обучающей выборки: {X_train.shape}\")\n",
        "print(f\"Размер тестовой выборки: {X_test.shape}\")\n",
        "print(f\"Количество признаков: {X_train.shape[1]}\")\n",
        "print(f\"Распределение классов в обучающей выборке:\")\n",
        "print(f\"  Класс 0 (злокачественная): {np.sum(y_train == 0)}\")\n",
        "print(f\"  Класс 1 (доброкачественная): {np.sum(y_train == 1)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def binary_cross_entropy_loss(y_pred, y_true):\n",
        "    epsilon = 1e-15\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "def binary_cross_entropy_grad(y_pred, y_true):\n",
        "    epsilon = 1e-15\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return (y_pred - y_true) / (y_pred * (1 - y_pred) + epsilon)\n",
        "\n",
        "def accuracy(y_pred, y_true):\n",
        "    predictions = (y_pred > 0.5).astype(int)\n",
        "    return np.mean(predictions == y_true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "network = NeuralNetwork([30, 16, 8, 1])\n",
        "\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "epochs = 200\n",
        "learning_rate = 0.1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    \n",
        "    for i in range(len(X_train)):\n",
        "        x = X_train[i]\n",
        "        y_true = y_train[i]\n",
        "        \n",
        "        y_pred = network.forward(x)\n",
        "        loss = binary_cross_entropy_loss(y_pred, y_true)\n",
        "        total_loss += loss\n",
        "        \n",
        "        grad = binary_cross_entropy_grad(y_pred, y_true)\n",
        "        network.backward(grad)\n",
        "        network.update_parameters(learning_rate)\n",
        "    \n",
        "    avg_loss = total_loss / len(X_train)\n",
        "    train_losses.append(avg_loss)\n",
        "    \n",
        "    train_preds = np.array([network.forward(x) for x in X_train])\n",
        "    train_acc = accuracy(train_preds, y_train)\n",
        "    train_accuracies.append(train_acc)\n",
        "    \n",
        "    test_preds = np.array([network.forward(x) for x in X_test])\n",
        "    test_acc = accuracy(test_preds, y_test)\n",
        "    test_accuracies.append(test_acc)\n",
        "    \n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "print(f\"\\nФинальная точность на обучающей выборке: {train_accuracies[-1]:.4f}\")\n",
        "print(f\"Финальная точность на тестовой выборке: {test_accuracies[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "axes[0].plot(train_losses)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Кривая обучения (Binary Cross-Entropy Loss)')\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].plot(train_accuracies, label='Train Accuracy')\n",
        "axes[1].plot(test_accuracies, label='Test Accuracy')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Точность модели')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
